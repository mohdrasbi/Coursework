{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 In-Class\n",
    "\n",
    "Today, we'll be working with *existing* implementations of the ML Classification approaches we've discussed so far (KNN, Naive Bayes, Logistic Regression). We're going to explore manipulating the various optimization features.\n",
    "\n",
    "## Your objective: Identifying the stability of an electrical grid\n",
    "\n",
    "Description of the dataset: https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+\n",
    "\n",
    "(We excluded the raw output variable)\n",
    "\n",
    "Your objective: Get the highest accuracy score you can, on any classifier, by manipulating the input to the classifier (either 1. editing the arguments to the classifier itself (e.g. the number of neighbors for KNN) or 2. scaling the training and testing features (e.g. the 0 - 1 scaling we discussed in class). \n",
    "\n",
    "## Documentation\n",
    "\n",
    "KNN - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "NB - https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "LR - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Scaling options\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['unstable'],\n",
       "       ['stable'],\n",
       "       ['unstable'],\n",
       "       ...,\n",
       "       ['stable'],\n",
       "       ['stable'],\n",
       "       ['unstable']], dtype='<U8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "trainFeatures = np.array(list(csv.reader(open('./GridStabilityTrainFeatures.csv', 'r'), delimiter=','))).astype('float')\n",
    "trainResults = np.array(list(csv.reader(open('./GridStabilityTrainResults.csv', 'r'), delimiter=',')))\n",
    "testFeatures = np.array(list(csv.reader(open('./GridStabilityTestFeatures.csv', 'r'), delimiter=','))).astype('float')\n",
    "testResults = np.array(list(csv.reader(open('./GridStabilityTestResults.csv', 'r'), delimiter=',')))\n",
    "\n",
    "trainResults\n",
    "\n",
    "# for i in range(len(trainFeatures)):\n",
    "#     scale = max(trainFeatures[:,i]) - min(trainFeatures[:,i])\n",
    "#     temp = []\n",
    "#     for j in range(len(trainFeatures[:,i])):\n",
    "#         temp.append(trainFeatures[j,i] - min(trainFeatures[:,i]) / scale)\n",
    "#     trainFeatures[:,i] = temp\n",
    "\n",
    "# for i in range(len(testFeatures)):\n",
    "#     scale = max(testFeatures[:,i]) - min(testFeatures[:,i])\n",
    "#     temp = []\n",
    "#     for j in range(len(testFeatures[:,i])):\n",
    "#         temp.append(testFeatures[j,i] - min(testFeatures[:,i]) / scale)\n",
    "#     testFeatures[:,i] = temp\n",
    "    \n",
    "\n",
    "# print(\"KNN Classifier\")\n",
    "# knn = KNeighborsClassifier(n_neighbors=12, weights='distance', algorithm='auto', leaf_size=30, p=1, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "# knn.fit(trainFeatures, trainResults)\n",
    "# print(knn.score(test2, testResults))\n",
    "\n",
    "# print(\"Naive Bayes Classifier\")\n",
    "# nb = GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "# nb.fit(trainFeatures, trainResults)\n",
    "# print(nb.score(testFeatures, testResults))\n",
    "\n",
    "# print(\"Logistic Regression\")\n",
    "# lr = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None)\n",
    "# lr.fit(trainFeatures, trainResults)\n",
    "# print(lr.score(testFeatures, testResults))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [0, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "a[:,0] = [0, 0]\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
